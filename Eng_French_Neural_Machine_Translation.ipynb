{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Eng_French_Neural_Machine_Translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcSUCVHBiKGu"
      },
      "source": [
        "## Neural Machine Translation  with attention practice on french to english DS\n",
        "\n",
        "### Code is heavily based on the tensorflow tutorial found at this link\n",
        "\n",
        "https://www.tensorflow.org/tutorials/text/nmt_with_attention\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg-HWN9-UWGb"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, LSTM, Softmax, Embedding\n",
        "from tensorflow.keras import Sequential\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "import unicodedata\n",
        "import os\n",
        "import re\n",
        "\n",
        "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n",
        "TF_FORCE_GPU_ALLOW_GROWTH=1\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRffQZ1w2ybu"
      },
      "source": [
        "os.chdir(\"./drive/MyDrive/Colab Notebooks\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqxn3-5VWdlr"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjs7ciuXWc3s"
      },
      "source": [
        "url = 'http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip'\n",
        "raw_data_path = tf.keras.utils.get_file(\n",
        "          \"eng_french_raw_data\",\n",
        "          url,\n",
        "          extract=True)\n",
        "\n",
        "data_path = os.path.join(os.path.dirname(raw_data_path), 'fra.txt')\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwSCqTc-W9wg"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0FMBagrYxLE"
      },
      "source": [
        "# preserve accents\n",
        "def normalize_unicode(s):\n",
        "  \"\"\" Used to preserve characters with accents \"\"\"\n",
        "  return ''.join(c for c in unicodedata.normalize('NFC', s))\n",
        "\n",
        "# delete accents\n",
        "def normalize_unicode(s):\n",
        "  \"\"\" Used to delete characters with accents \"\"\"\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def read_file(path):\n",
        "  with open(path, \"r\", encoding=\"UTF-8\") as f:\n",
        "    return f.read()\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  # punctuations to preserve (\".\", \"?\", \"!\", \",\")\n",
        "  \n",
        "  # lower the case\n",
        "  w = w.lower()\n",
        "  \n",
        "  # normalize unicode \n",
        "  w = normalize_unicode(w)\n",
        "\n",
        "  # padding punctuation with spaces\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([.!,?])\", r\" \\1 \", w)\n",
        "\n",
        "  # replace long white spaces with a single space\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, punctuations to save)\n",
        "  #w = re.sub(r\"[^a-z.!,'?]+\", \" \", w)\n",
        "\n",
        "  # remove strip trailing and ending spaces\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w\n",
        "\n",
        "\n",
        "def tokenize(list_of_sentences):\n",
        "  tokenizer = Tokenizer(filters='',\n",
        "                        split=' ', \n",
        "                        char_level=False,\n",
        "                        oov_token=\"<oov>\")\n",
        "  tokenizer.fit_on_texts(list_of_sentences)\n",
        "  tokenized_sentences = tokenizer.texts_to_sequences(list_of_sentences)\n",
        "  tokenized_sentences = pad_sequences(tokenized_sentences, padding='post')\n",
        "\n",
        "  return tokenized_sentences, tokenizer\n",
        "\n",
        "\n",
        "def preprocess_data(path, train_split=0.8, dataset_fraction=0.4):\n",
        "  \"\"\"\n",
        "    preprocess data\n",
        "\n",
        "    Adjust dataset_fraction to increase or reduce dataset and subsequently\n",
        "    vocabulary size depending on compute resources\n",
        "\n",
        "    Args:\n",
        "      path (str): path to text file / data\n",
        "      train_split (float): train/test split\n",
        "      dataset_fraction (float): fraction of the full dataset to train on\n",
        "\n",
        "    Return:\n",
        "      (tuple): lists with train and test sentences of source and target \n",
        "                languages with their tokenizers \n",
        "  \"\"\"\n",
        "  # read data \n",
        "  input_data_as_string = read_file(path)\n",
        "\n",
        "  # preprocess data for tokenizer - list of list of sentences\n",
        "  input_data = [[preprocess_sentence(sentence) for sentence in line.split(\"\\t\")] \\\n",
        "                  for line in input_data_as_string.split(\"\\n\")\\\n",
        "                  if len(line.split(\"\\t\")) == 2]\n",
        "\n",
        "  english_sentences, target_lang_sentences = list(zip(*input_data))\n",
        "  size_of_data = len(english_sentences)\n",
        "  data_indices = np.arange(size_of_data)\n",
        "  np.random.shuffle(data_indices)\n",
        "  data_indices = data_indices[:int(size_of_data * dataset_fraction)]\n",
        "  split_point = int(train_split * len(data_indices))\n",
        "  train_idx, test_idx = data_indices[:split_point], data_indices[split_point:]\n",
        "  \n",
        "  # tokenize\n",
        "  tokenized_english_sentences, english_tokenizer = tokenize(english_sentences)\n",
        "  tokenized_target_lang_sentences, target_lang_tokenizer = tokenize(target_lang_sentences)\n",
        "\n",
        "  return  tokenized_english_sentences[train_idx], \\\n",
        "          tokenized_english_sentences[test_idx], \\\n",
        "          tokenized_target_lang_sentences[train_idx], \\\n",
        "          tokenized_target_lang_sentences[test_idx], \\\n",
        "          english_tokenizer, \\\n",
        "          target_lang_tokenizer\n",
        "\n",
        "\n",
        "def display_sample_sentences(num_of_samples, \n",
        "                             tokenized_english_sentences,\n",
        "                             tokenized_target_lang_sentences,\n",
        "                             english_tokenizer,\n",
        "                             target_lang_tokenizer,\n",
        "                             target_lang=\"French\"):\n",
        "  num_of_words_not_oov = np.sum(tokenized_english_sentences != 0, axis=1)\n",
        "  indices_of_top_words_in_vocab = np.argsort(num_of_words_not_oov)[::-1][:num_of_samples]\n",
        "  sample_english_tokens = tokenized_english_sentences[indices_of_top_words_in_vocab]\n",
        "  sample_target_lang_tokens = tokenized_target_lang_sentences[indices_of_top_words_in_vocab]\n",
        "  sample_english_sentences = english_tokenizer.sequences_to_texts(sample_english_tokens)\n",
        "  sample_target_lang_sentences = target_lang_tokenizer.sequences_to_texts(sample_target_lang_tokens)\n",
        "  for eng, targ_lang in zip(sample_english_sentences, sample_target_lang_sentences):\n",
        "    print(f\"English: {eng}\")\n",
        "    print(f\"{target_lang}: {targ_lang} \\n\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1aXrx7B8wFc"
      },
      "source": [
        "train_eng_sen, test_eng_sen, train_french_sen, test_french_sen,\\\n",
        "          english_tokenizer, french_tokenizer = preprocess_data(data_path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGf1vxN1B3AO",
        "outputId": "584e52ba-28ba-43da-a65b-82b8481d12e5"
      },
      "source": [
        "display_sample_sentences(3,\n",
        "                         train_eng_sen,\n",
        "                         train_french_sen,\n",
        "                         english_tokenizer,\n",
        "                         french_tokenizer)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English: <start> top-down economics never works , said obama . the country does not succeed when just those at the very top are doing well . we succeed when the middle class gets bigger , when it feels greater security . <end> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov>\n",
            "French: <start> « l'economie en partant du haut vers le bas , ca ne marche jamais , » a dit obama . « le pays ne reussit pas lorsque seulement ceux qui sont au sommet s'en sortent bien . nous reussissons lorsque la classe moyenne s'elargit , lorsqu'elle se sent davantage en securite . » <end> <oov> <oov> <oov> <oov> <oov> \n",
            "\n",
            "English: <start> a carbon footprint is the amount of carbon dioxide pollution that we produce as a result of our activities . some people try to reduce their carbon footprint because they are concerned about climate change . <end> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov>\n",
            "French: <start> une empreinte carbone est la somme de pollution au dioxyde de carbone que nous produisons par nos activites . certaines personnes essaient de reduire leur empreinte carbone parce qu'elles sont inquietes du changement climatique . <end> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> \n",
            "\n",
            "English: <start> even at the end of the nineteenth century , sailors in the british navy were not permitted to use knives and forks because using them was considered a sign of weakness . <end> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov>\n",
            "French: <start> meme a la fin du dix-neuvieme siecle , les marins de la marine britannique n'etaient pas autorises a utiliser des couteaux et des fourchettes parce que c'etait considere comme un signe de faiblesse . <end> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeV3Q-2rZl-7"
      },
      "source": [
        "## Batch Data and Hyperameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8jvzkOEZxGP"
      },
      "source": [
        "num_train, NUM_INP_TIMESTEPS = train_eng_sen.shape\n",
        "_, NUM_TARG_TIMESTEPS = test_eng_sen.shape\n",
        "BUFFER_SIZE = num_train\n",
        "BATCH_SIZE = 32\n",
        "STEPS_PER_EPOCH = num_train//BATCH_SIZE\n",
        "EMBED_DIMS = 256\n",
        "UNITS = 1024\n",
        "ATTENTION_DIMS = 10\n",
        "VOCAB_INP_SIZE = len(english_tokenizer.word_index) + 1 #to account for padding\n",
        "VOCAB_TARG_SIZE = len(french_tokenizer.word_index) + 1\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 30\n",
        "\n",
        "sparse_cat_lossfn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_eng_sen, train_french_sen)).shuffle(BUFFER_SIZE)\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_eng_sen, test_french_sen)).shuffle(BUFFER_SIZE)\n",
        "test_data = test_data.batch(BATCH_SIZE)\n",
        "\n",
        "def loss_fn(targets, outputs, sparse_cat_lossfn=sparse_cat_lossfn):\n",
        "  loss = sparse_cat_lossfn(targets, outputs)\n",
        "  mask = tf.cast(targets != 0, loss.dtype)\n",
        "  return tf.reduce_mean(loss * mask)\n",
        "\n",
        "def disp_sentence(x):\n",
        "   return \" \".join(i for i in x.split(\" \") if i not in ['<start>', '<oov>', '<end>'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1gknCCSZt0n"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBtWVIy5Z6de"
      },
      "source": [
        "class AttentionLayer(tf.keras.Model):\n",
        "  def __init__(self, attention_hidden_dims):\n",
        "    super(AttentionLayer, self).__init__()\n",
        "    self.W1 = Dense(attention_hidden_dims)\n",
        "    self.W2 = Dense(attention_hidden_dims)\n",
        "    self.V = Dense(1)\n",
        "    self.softmax = Softmax(axis=1)\n",
        "\n",
        "  def call(self, query_decoder, key_encoder):\n",
        "    query_decoder = tf.expand_dims(query_decoder, 1)\n",
        "    o = tf.keras.activations.tanh(\n",
        "            self.W1(query_decoder) + self.W2(key_encoder))\n",
        "    o = self.V(o)\n",
        "    attention_weights = self.softmax(o)\n",
        "    temp = tf.reduce_sum(attention_weights * key_encoder, axis=1)\n",
        "    context_vector = tf.expand_dims(temp, axis=1)\n",
        "    return attention_weights, context_vector\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, units, vocab_size, emb_dims):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.lstm_layer = LSTM(units, return_state=True, return_sequences=True)\n",
        "    self.embedding_layer = Embedding(vocab_size, \n",
        "                                     emb_dims, \n",
        "                                     mask_zero=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    inputs = self.embedding_layer(inputs)\n",
        "    hidden_state_sequence, final_hidden_state, final_cell_state = self.lstm_layer(inputs)\n",
        "    final_states = [final_hidden_state, final_cell_state]\n",
        "    return hidden_state_sequence, final_states\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, units, vocab_size, emb_dims):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.lstm_layer = LSTM(units, return_state=True, return_sequences=True)\n",
        "    self.embedding_layer = Embedding(vocab_size, \n",
        "                                     emb_dims, \n",
        "                                     mask_zero=True)\n",
        "    self.output_layer = Dense(vocab_size)\n",
        "\n",
        "  def call(self, \n",
        "           inputs, \n",
        "           context_vector,\n",
        "           previous_states):\n",
        "\n",
        "    if len(inputs.shape) == 1:\n",
        "      inputs = tf.expand_dims(inputs, axis=1)\n",
        "    inputs = self.embedding_layer(inputs)\n",
        "    inputs = tf.concat([inputs, context_vector], axis=-1)\n",
        "    _, next_hidden_state, next_cell_state = self.lstm_layer(inputs, \\\n",
        "                                            initial_state=previous_states)\n",
        "    output = self.output_layer(next_hidden_state)\n",
        "    next_states = [next_hidden_state, next_cell_state]\n",
        "    return next_states, output\n",
        "\n",
        "\n",
        "class NMT(tf.keras.Model):\n",
        "  def __init__(self, units, \n",
        "               enc_vocab_size, \n",
        "               dec_vocab_size,\n",
        "               emb_dims, \n",
        "               attention_hidden_dims, \n",
        "               decoder_tokenizer):\n",
        "    \n",
        "    super(NMT, self).__init__()\n",
        "    self.units = units\n",
        "    self.encoder = Encoder(units, enc_vocab_size, emb_dims)\n",
        "    self.decoder = Decoder(units, dec_vocab_size, emb_dims)\n",
        "    self.attention_layer = AttentionLayer(attention_hidden_dims)\n",
        "    self.decoder_tokenizer = decoder_tokenizer\n",
        "\n",
        "  def call(self, \n",
        "           model_inputs,\n",
        "           max_timesteps = NUM_TARG_TIMESTEPS,\n",
        "           eval=False):\n",
        "    \n",
        "      inputs, targets = model_inputs\n",
        "      bs, _ = inputs.shape\n",
        "      loss = 0\n",
        "      hidden_state_sequence, input_states = self.encoder(inputs)\n",
        "      query_decoder = input_states[0] # select hidden state\n",
        "      predicted_outputs = []\n",
        "      attention_weights_list = []\n",
        "      \n",
        "      dec_input = self.decoder_tokenizer.texts_to_sequences(['<start>'])\n",
        "      dec_input = tf.constant(dec_input * bs)      \n",
        "\n",
        "      for i in range(1, max_timesteps):\n",
        "        attention_weights, context_vector = self.attention_layer(query_decoder, hidden_state_sequence)\n",
        "        input_states, outputs = self.decoder(dec_input, context_vector, input_states)\n",
        "        if eval:\n",
        "          dec_input = tf.argmax(outputs, axis=1)\n",
        "        else:\n",
        "          dec_input = targets[:, i] # dec input for next stage is target for this stage\n",
        "\n",
        "        query_decoder = input_states[0]\n",
        "        loss += loss_fn(dec_input, outputs)\n",
        "        \n",
        "        \n",
        "        if eval:\n",
        "          predictions = tf.expand_dims(dec_input, 1)\n",
        "          predictions = self.decoder_tokenizer.sequences_to_texts(predictions.numpy())\n",
        "          predicted_outputs.append(predictions)\n",
        "          attention_weights_list.append(attention_weights.numpy())\n",
        "\n",
        "      loss /= max_timesteps\n",
        "\n",
        "      if len(attention_weights_list) != 0:\n",
        "        predicted_outputs = np.array(predicted_outputs).T\n",
        "        attention_weights = np.stack(attention_weights_list, axis=2)[..., -1]\n",
        "                                      \n",
        "      return loss, predicted_outputs, attention_weights\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P6Lh2Dku5i2"
      },
      "source": [
        "sparse_cat_lossfn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_model(model, optimizer, inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss, prediction, attention_weights = model((inputs, targets))\n",
        "\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return loss, prediction, attention_weights\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMmbHBzUuZKQ"
      },
      "source": [
        "## Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olKX672luYUx"
      },
      "source": [
        "checkpoint_directory = \"./checkpoints\"\n",
        "stats_save_path = os.path.join(checkpoint_directory, \"train_stats.pkl\")\n",
        "\n",
        "def save_pickled_data(data, data_path):\n",
        "    \"\"\"Saves a pickle file\"\"\"\n",
        "    if os.path.exists(data_path):\n",
        "      os.remove(data_path)\n",
        "\n",
        "    with open(data_path, \"wb\") as f:\n",
        "        data = pickle.dump(data, f)\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_pickled_data(data_path):\n",
        "    \"\"\"Reads a pickle file\"\"\"\n",
        "    with open(data_path, \"rb\") as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
        "nmt_model = NMT(UNITS, VOCAB_INP_SIZE, VOCAB_TARG_SIZE, EMBED_DIMS, ATTENTION_DIMS, french_tokenizer)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, nmt_model=nmt_model)\n",
        "manager = tf.train.CheckpointManager(\n",
        "    checkpoint, directory=checkpoint_directory, max_to_keep=2)\n",
        "\n",
        "status = checkpoint.restore(manager.latest_checkpoint)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vTW86SCx32U",
        "outputId": "9dcb5752-9d56-42eb-efac-2bd8bd0392bf"
      },
      "source": [
        "resume_epoch = max([1] + [int(i.split('.')[0].split(\"-\")[1]) \\\n",
        "                          for i in os.listdir('checkpoints') \\\n",
        "                          if 'ckpt' in i])\n",
        "train_stats = []\n",
        "for epoch in range(resume_epoch, EPOCHS + 1):\n",
        "  train_epoch_loss = []\n",
        "  val_epoch_loss = []\n",
        "  attention_weights_list = []\n",
        "  prediction_list = []\n",
        "  for i, (inp, targ) in enumerate(train_data):\n",
        "    loss, _, _ = train_model(nmt_model, optimizer, inp, targ)\n",
        "    train_epoch_loss.append(loss.numpy())\n",
        "\n",
        "  for i, (inp, targ) in enumerate(test_data):\n",
        "    loss, prediction, attention_weights = nmt_model((inp, targ), eval=True)\n",
        "    attention_weights_list.append(attention_weights)\n",
        "    prediction_list.append(prediction)\n",
        "    val_epoch_loss.append(loss.numpy())\n",
        "  \n",
        "  inp_text = english_tokenizer.sequences_to_texts(inp.numpy())\n",
        "  targ_text = nmt_model.decoder_tokenizer.sequences_to_texts(targ.numpy())\n",
        "\n",
        "  #status.assert_consumed()  # Optional sanity checks.\n",
        "  manager.save()\n",
        "  \n",
        "  if epoch % 5 == 1:\n",
        "    for idx in np.random.randint(0, len(inp_text), 5):\n",
        "      pred = list(prediction[idx])\n",
        "      pred_index = pred.index('<end>') if '<end>' in pred else len(pred)\n",
        "      prediction_text = \" \".join(pred[:pred_index])\n",
        "      print(f\"input: {disp_sentence(inp_text[idx])}\")\n",
        "      print(f\"target: {disp_sentence(targ_text[idx])}\")\n",
        "      print(f\"prediction: {prediction_text} \\n\")\n",
        "  \n",
        "  train_epoch_loss = np.mean(train_epoch_loss)\n",
        "  val_epoch_loss = np.mean(val_epoch_loss)\n",
        "  train_stats.append({'epoch': epoch, 'train_loss':train_epoch_loss, 'val_loss':val_epoch_loss})\n",
        "  save_pickled_data(train_stats, stats_save_path)\n",
        "  print(f\"Epoch {epoch}, train loss: {train_epoch_loss} val loss: {val_epoch_loss}\\n\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <bound method Tokenizer.texts_to_sequences_generator of <keras_preprocessing.text.Tokenizer object at 0x7f9b914c3400>> appears to be a generator function. It will not be converted by AutoGraph.\n",
            "WARNING: Entity <bound method Tokenizer.texts_to_sequences_generator of <keras_preprocessing.text.Tokenizer object at 0x7f9b914c3400>> appears to be a generator function. It will not be converted by AutoGraph.\n",
            "input: he constantly criticizes other people .\n",
            "target: il est toujours en train de critiquer les autres .\n",
            "prediction: il a ete arrete de la mort . \n",
            "\n",
            "input: that's not your fault .\n",
            "target: ce n'est pas de ta faute .\n",
            "prediction: ce n'est pas ton affaire . \n",
            "\n",
            "input: let me get you the key .\n",
            "target: laissez-moi vous degoter la cle .\n",
            "prediction: laissez-moi te donner une minute . \n",
            "\n",
            "input: he constantly criticizes other people .\n",
            "target: il est toujours en train de critiquer les autres .\n",
            "prediction: il a ete arrete de la mort . \n",
            "\n",
            "input: he constantly criticizes other people .\n",
            "target: il est toujours en train de critiquer les autres .\n",
            "prediction: il a ete arrete de la mort . \n",
            "\n",
            "Epoch 1, train loss: 0.5171830654144287 val loss: 0.6077547669410706\n",
            "\n",
            "Epoch 2, train loss: 0.3890637457370758 val loss: 0.5316329002380371\n",
            "\n",
            "Epoch 3, train loss: 0.29100582003593445 val loss: 0.4404694139957428\n",
            "\n",
            "Epoch 4, train loss: 0.21251094341278076 val loss: 0.28914323449134827\n",
            "\n",
            "Epoch 5, train loss: 0.15326382219791412 val loss: 0.28346899151802063\n",
            "\n",
            "input: i owe you so much .\n",
            "target: je te dois tant .\n",
            "prediction: je te dois bien . \n",
            "\n",
            "input: our eyes take time to adjust to the dark .\n",
            "target: nos yeux prennent du temps pour s'adapter au noir .\n",
            "prediction: nos enfants vont a la facon pour parler du temps . \n",
            "\n",
            "input: they all looked relieved .\n",
            "target: elles ont toutes eu l'air soulage .\n",
            "prediction: ils ont l'air tous satisfaits . \n",
            "\n",
            "input: you don't have to say anything if you don't feel like it .\n",
            "target: t’as pas a dire quoi que ce soit si tu ne preferes pas .\n",
            "prediction: tu n’as pas a dire quoi que ce soit qui tu ne le penses . \n",
            "\n",
            "input: could you bring us the bill , please ?\n",
            "target: j'aimerais la note , je vous prie .\n",
            "prediction: pouvez-vous epeler ca , s'il te plait , les mecs ? \n",
            "\n",
            "Epoch 6, train loss: 0.11046382784843445 val loss: 0.1883246749639511\n",
            "\n",
            "Epoch 7, train loss: 0.07961501181125641 val loss: 0.22705227136611938\n",
            "\n",
            "Epoch 8, train loss: 0.05828756093978882 val loss: 0.2608311176300049\n",
            "\n",
            "Epoch 9, train loss: 0.0443727970123291 val loss: 0.20310074090957642\n",
            "\n",
            "Epoch 10, train loss: 0.03578756004571915 val loss: 0.1702568680047989\n",
            "\n",
            "input: i called him tom .\n",
            "target: je l'ai appele tom .\n",
            "prediction: je l'ai appele a tom . \n",
            "\n",
            "input: you're a talented kid .\n",
            "target: vous etes une enfant talentueuse .\n",
            "prediction: tu es un gamin de dents . \n",
            "\n",
            "input: these keys are not mine .\n",
            "target: ces cles ne sont pas a moi .\n",
            "prediction: ces chaussures sont a la religion . \n",
            "\n",
            "input: never say never .\n",
            "target: il ne faut jamais dire jamais .\n",
            "prediction: ne le dis jamais a qui que ce soit . \n",
            "\n",
            "input: how did things turn out ?\n",
            "target: comment les choses ont-elles tourne ?\n",
            "prediction: comment se passe , les autres ? \n",
            "\n",
            "Epoch 11, train loss: 0.03051801398396492 val loss: 0.1599748283624649\n",
            "\n",
            "Epoch 12, train loss: 0.0273111741989851 val loss: 0.12475882470607758\n",
            "\n",
            "Epoch 13, train loss: 0.025255484506487846 val loss: 0.1186259314417839\n",
            "\n",
            "Epoch 14, train loss: 0.02374337613582611 val loss: 0.1474461704492569\n",
            "\n",
            "Epoch 15, train loss: 0.022761566564440727 val loss: 0.14995542168617249\n",
            "\n",
            "input: tom threatened mary with a knife .\n",
            "target: tom a menace marie avec un couteau .\n",
            "prediction: tom a achete une augmentation a mary . \n",
            "\n",
            "input: i hated school .\n",
            "target: je detestais l'ecole .\n",
            "prediction: je me suis douche a l'ecole . \n",
            "\n",
            "input: don't give up the fight .\n",
            "target: n'abandonnez pas le combat !\n",
            "prediction: n'abandonne pas le combat ! \n",
            "\n",
            "input: my wife was mad .\n",
            "target: ma femme etait folle .\n",
            "prediction: ma femme etait en fleurs . \n",
            "\n",
            "input: we should hang out sometime .\n",
            "target: on devrait sortir ensemble , un de ces quatre .\n",
            "prediction: nous devrions nous promener . \n",
            "\n",
            "Epoch 16, train loss: 0.021857179701328278 val loss: 0.11755339056253433\n",
            "\n",
            "Epoch 17, train loss: 0.020773451775312424 val loss: 0.13916561007499695\n",
            "\n",
            "Epoch 18, train loss: 0.02034148760139942 val loss: 0.11884742230176926\n",
            "\n",
            "Epoch 19, train loss: 0.019904209300875664 val loss: 0.11450677365064621\n",
            "\n",
            "Epoch 20, train loss: 0.019315486773848534 val loss: 0.11554335802793503\n",
            "\n",
            "input: why do you need a doctor ?\n",
            "target: pourquoi as-tu besoin d'un docteur ?\n",
            "prediction: pourquoi lui avez-vous besoin d'une medecin ? \n",
            "\n",
            "input: didn't you feel like going ?\n",
            "target: n'aviez-vous pas envie d'y aller ?\n",
            "prediction: n'avais-tu pas envie d'y aller ? \n",
            "\n",
            "input: this boy is my son .\n",
            "target: ce garcon est mon fils .\n",
            "prediction: ce garcon est mon fils . \n",
            "\n",
            "input: why do you need a doctor ?\n",
            "target: pourquoi as-tu besoin d'un docteur ?\n",
            "prediction: pourquoi lui avez-vous besoin d'une medecin ? \n",
            "\n",
            "input: why don't you sit down ?\n",
            "target: pourquoi ne vous asseyez-vous pas ?\n",
            "prediction: pourquoi ne prenez-vous pas conge ? \n",
            "\n",
            "Epoch 21, train loss: 0.018992198631167412 val loss: 0.10821209102869034\n",
            "\n",
            "Epoch 22, train loss: 0.018464822322130203 val loss: 0.12540769577026367\n",
            "\n",
            "Epoch 23, train loss: 0.018269414082169533 val loss: 0.11621483415365219\n",
            "\n",
            "Epoch 24, train loss: 0.01780519261956215 val loss: 0.12309535592794418\n",
            "\n",
            "Epoch 25, train loss: 0.01776038482785225 val loss: 0.12270139902830124\n",
            "\n",
            "input: don't make a decision right now .\n",
            "target: ne prends pas de decision immediatement !\n",
            "prediction: ne commets pas de travail ! \n",
            "\n",
            "input: don't make a decision right now .\n",
            "target: ne prends pas de decision immediatement !\n",
            "prediction: ne commets pas de travail ! \n",
            "\n",
            "input: don't point your gun at anyone .\n",
            "target: ne pointe pas ton arme en direction de qui que ce soit !\n",
            "prediction: ne pointe pas ton arme vers quiconque ! \n",
            "\n",
            "input: why not break the door down ?\n",
            "target: pourquoi ne pas enfoncer la porte  ?\n",
            "prediction: pourquoi ne pas porte le chat ? \n",
            "\n",
            "input: tom found his new job interesting .\n",
            "target: tom trouvait son nouveau travail interessant .\n",
            "prediction: tom a trouve son travail a noel . \n",
            "\n",
            "Epoch 26, train loss: 0.017406770959496498 val loss: 0.08739567548036575\n",
            "\n",
            "Epoch 27, train loss: 0.01704905927181244 val loss: 0.1258585900068283\n",
            "\n",
            "Epoch 28, train loss: 0.01712576486170292 val loss: 0.10269440710544586\n",
            "\n",
            "Epoch 29, train loss: 0.01651221327483654 val loss: 0.08163686841726303\n",
            "\n",
            "Epoch 30, train loss: 0.016549106687307358 val loss: 0.1220378503203392\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr0VxEdN0uAF"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkRUf8X7zovg"
      },
      "source": [
        "def prep_text_for_eval(x, tokenizer=english_tokenizer):\n",
        "  list_of_sentences = x.split(\".\")\n",
        "  list_of_sentences = [sen + \".\" for sen in list_of_sentences]\n",
        "  inp = [preprocess_sentence(i) for i in list_of_sentences]\n",
        "  inp = tokenizer.texts_to_sequences(inp)\n",
        "  inp = pad_sequences(inp, maxlen= NUM_INP_TIMESTEPS, padding='post')\n",
        "  return inp\n",
        "\n",
        "def display_prediction(prediction):\n",
        "  pred = []\n",
        "  for line in prediction:\n",
        "    pred_index = np.where(line == '<end>')[0][0]\n",
        "    pred.extend(list(line[:pred_index]))\n",
        "  return \" \".join(pred)\n",
        "\n",
        "def query_preprocessor(inp, english_tokenizer=english_tokenizer):\n",
        "  if tf.is_tensor(inp):\n",
        "    query = english_tokenizer.sequences_to_texts(inp.numpy())\n",
        "    query = np.array(query)\n",
        "  elif type(inp) is np.ndarray and np.issubdtype(inp.dtype, np.integer):\n",
        "    query = english_tokenizer.sequences_to_texts(inp)\n",
        "    \n",
        "  return query"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6yxxZWT-ofZ"
      },
      "source": [
        "def display_attention(query, attention, prediction, base_size):\n",
        "  \n",
        "  num_plots, query_size, pred_size = attention.shape\n",
        "  \n",
        "  num_rows = num_plots//2 if num_plots % 2 == 0 else num_plots//2 + 1\n",
        "  rows = base_size * num_rows\n",
        "  cols = 2 * base_size\n",
        "  fig = plt.figure(figsize=(rows, cols))\n",
        "  \n",
        "  input_sentences = []\n",
        "  for i in range(1, num_plots + 1):\n",
        "    \n",
        "    pred = prediction[i-1]\n",
        "    pred_index = np.where(pred == '<end>')[0][0]\n",
        "    pred = pred[:pred_index]\n",
        "    \n",
        "    \n",
        "    que = query[i-1].split(\" \")\n",
        "    query_index = que.index('<end>') if '<end>' in que else len(que)\n",
        "    que = np.array(que)[1:query_index]\n",
        "    input_sentences.append(\" \".join(que))\n",
        "    \n",
        "    att_matrix = attention[i-1]\n",
        "    att_matrix = att_matrix[1:query_index,:pred_index]\n",
        "    \n",
        "\n",
        "    ax = fig.add_subplot(num_rows, 2, i)\n",
        "    ax.matshow(att_matrix, cmap='viridis')\n",
        "    ax.set_xticks(np.arange(len(pred)))\n",
        "    ax.set_yticks(np.arange(len(que)))\n",
        "    ax.set_xticklabels(pred)\n",
        "    ax.set_yticklabels(que)\n",
        "\n",
        "  input_sentences = \" \".join(input_sentences)\n",
        "  print(f\"input: {input_sentences}\")\n",
        "  plt.show()\n",
        "\n",
        "def prep_and_eval(inp, targ=None, base_size=10):\n",
        "  if type(inp) is str:\n",
        "    inp = prep_text_for_eval(inp)\n",
        "\n",
        "  _, prediction, attention = nmt_model((inp, targ), eval=True)\n",
        "  query = query_preprocessor(inp)\n",
        "  print(f\"prediction: {display_prediction(prediction)}\")\n",
        "  display_attention(query, attention, prediction, base_size)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKXbNo95Cj_r"
      },
      "source": [
        "## Random Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "u-47bsrH0nYZ",
        "outputId": "307f6fab-cd02-4430-ac68-b4932c48c9b4"
      },
      "source": [
        "input_sentence = \"He is a honest man\"\n",
        "prep_and_eval(input_sentence)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction: c'est un homme honnete .\n",
            "input: he is a honest man .\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc8x3mDJ9IpG"
      },
      "source": [
        "test_data_ = test_data.take(5)\n",
        "sample_test_data = list(test_data.as_numpy_iterator())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm376gFrCnnq"
      },
      "source": [
        "## Test set input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "d4MLFy729v4O",
        "outputId": "d3a7e385-92fb-4b02-ef10-b610b5846b49"
      },
      "source": [
        "idx = 6\n",
        "num_samples = 1\n",
        "inp, targ = sample_test_data[idx]\n",
        "inp = inp[:num_samples]\n",
        "targ = inp[:num_samples]\n",
        "prep_and_eval(inp, targ, 15)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction: c'etait une erreur .\n",
            "input: it was a mistake .\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbkAAAH+CAYAAAABCXD8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARdElEQVR4nO3df8zudV3H8dcbDnD4Jc5fLZeTphKCE8oDKcZmaDaNTExjE+co5slWWRpuVGZmPyS12lorZbWh5cpQK1dOM9QgQBTkh6LM5siVc6ZiCv4ghHd/3Bd5ezqHc4hzn+u+3vfjsbH7e13f6/u939eHw57ne13XfVPdHQCY6KBlDwAAG0XkABhL5AAYS+QAGEvkABhL5AAYS+QOoKp6VVWdu5fHPLuqTtiHc724ql642D63qh6+n8YEGEPkNp9nJ9lr5Lr7Dd395sXNc5OIHCuhqg6+t9v/j/Ntu38TMZnIbaCqemFV3VhVN1TVnye5PcnXF/seVVXvrqprq+ryqjq+qk5L8qwkr6uq6xePeVFVfXhxjrdX1RGL419VVedX1XOT7EjylsUxhy/r+a6Cqjq2qj627vb5i7X8QFX9blV9qKo+WVWnL/YfXFWvW/w7uLGqfnp506+GqnrBYh2vr6o3Ltbw9qr6vaq6IcmTdnP7/xyzONft68773Kq6eLF9cVW9oaquTvLaZTxPVoPIbZCqOjHJK5Kc0d0nJfmF7n59d7918ZCLkvx8dz8hyflJ/ri7r0zyziQv7+6Tu/tTSd7R3acszvGJJOet/z7d/bYk1yQ5Z3HM1w/MMxxpW3efmuQXk/z64r7zkny5u09JckqSF1XVdy9rwM2uqh6b5OwkT+7uk5PcleScJEcmubq7T+ruf1l/O8kX93DM3nxXktO6+2Ub8FQYwmX+xjkjySXd/YUk6e5b79lRVUclOS3JJVV1z92H7eE8j6uq30rywCRHJXnPhk3MOxZfr01y7GL76Ukev7hiTpJjkjwmyS0HdrSV8dQkT0jy4cWf7cOT/GfWwvX2dY9bf3tPx+zNJd191/4Zm6lEbjkOSvJfi7+17s3FSZ7d3TcsPrTylA2cayv4Zr79FYzt67bvWHy9K9/6b6OydsXtLxf7ppK8qbt/+dvurDp/lyB9Y93t3R6zsP6X627fZd9X7/e0jOflyo3zviTPq6oHJ0lVPeieHd39lSS3VNXzFvuqqk5a7L4tydHrznN0ks9W1SHZ80s4ux7Dnn0uycOq6sFVdViSM/fy+Pck+ZnF+qeqjquqIzd6yBV2aZLnVtXDkrU/91X1yPtxzOeq6rFVdVCSszZsasYSuQ3S3Tcl+e0k/7x4c/33d3nIOUnOW+y7KcmPLe7/qyQvr6rrqupRSX4tydVJrkhy8x6+3cVJ3uCDJ3vX3XcmeXWSDyV5b/a8pvf40yQfT/KRxQdW3hivgOxRd388a+9F/2NV3Zi1Nf7O+3HMBUn+PsmVST67UXNvdVX1rqk/hlT+VzsATOVKDoCxRA6AsUQOgLFEDoCxRA6AsURuE6qqncueYauy9stj7Zdn8tqL3OY09g/cCrD2y2Ptl2fs2oscAGON/GHwQ+uw3p7V/c1Ld+aOHLLH39fMRrL2y2Ptl2fV1/62fOkL3f3Q3e0b+euJtufIfH89ddljwIH1rf+jBWwp/3T3JZ/e0z4vVwIwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATDWSkSuqq5cfD22qp6/7HkAWA0rEbnuPm2xeWwSkQNgn6xE5Krq9sXmhUlOr6rrq+qly5wJgM1v27IHuI8uSHJ+d5+5646q2plkZ5JszxEHei4ANqGVuJLbF919UXfv6O4dh+SwZY8DwCYwJnIAsKtVi9xtSY5e9hAArIZVi9yNSe6qqht88ASAvVmJD55091GLr3cmOWPJ4wCwIlbtSg4A9pnIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATDWtmUPsGEOOnjZE8CBdfddy54ANh1XcgCMJXIAjCVyAIwlcgCMJXIAjCVyAIwlcgCMJXIAjCVyAIwlcgCMJXIAjCVyAIwlcgCMJXIAjCVyAIwlcgCMJXIAjCVyAIwlcgCMJXIAjCVyAIwlcgCMJXIAjCVyAIwlcgCMJXIAjCVyAIwlcgCMJXIAjCVyAIwlcgCMJXIAjCVyAIwlcgCMJXIAjCVyAIwlcgCMJXIAjCVyAIwlcgCMJXIAjCVyAIwlcgCMJXIAjCVyAIwlcgCMdUAjV1Uvr6qXLLb/oKret9g+o6reUlV/UlXXVNVNVfUb6467sKo+XlU3VtXrD+TMAKyubQf4+12e5JeS/GGSHUkOq6pDkpye5LIkl3T3rVV1cJJLq+rxST6T5Kwkx3d3V9UDd3fiqtqZZGeSbM8RG/9MANj0DvTLldcmeUJVPSDJHUmuylrsTs9aAH+iqj6S5LokJyY5IcmXk3wjyZ9V1XOSfG13J+7ui7p7R3fvOCSHbfwzAWDTO6CR6+47k9yS5NwkV2YtbD+Y5NFJvp7k/CRP7e7HJ/mHJNu7+5tJTk3ytiRnJnn3gZwZgNW1jA+eXJ61mF222H5x1q7cHpDkq0m+XFXfkeQZSVJVRyU5prvfleSlSU5awswArKAD/Z5csha2X01yVXd/taq+keTy7r6hqq5LcnOSf09yxeLxRyf5u6ranqSSvGwJMwOwgg545Lr70iSHrLt93Lrtc/dw2KkbPBYAA/k5OQDGEjkAxhI5AMYSOQDGEjkAxhI5AMYSOQDGEjkAxhI5AMYSOQDGEjkAxhI5AMYSOQDGEjkAxhI5AMYSOQDGEjkAxhI5AMYSOQDGEjkAxhI5AMYSOQDGEjkAxhI5AMYSOQDGEjkAxhI5AMYSOQDGEjkAxhI5AMYSOQDGEjkAxhI5AMYSOQDGEjkAxhI5AMYSOQDGEjkAxhI5AMYSOQDGEjkAxtq27AE2TN+97Am2pC+98InLHmHLesjf3LTsEba0u267bdkjbF29512u5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYa2UiV1V/W1XXVtVNVbVz2fMAsPltW/YA98FPdfetVXV4kg9X1du7+4vLHgqAzWuVIveSqjprsf2IJI9J8r+RW1zd7UyS7TniwE8HwKazEpGrqqckeVqSJ3X316rqA0m2r39Md1+U5KIkeUA9qA/0jABsPqvyntwxSb60CNzxSZ647IEA2PxWJXLvTrKtqj6R5MIkH1zyPACsgJV4ubK770jyjGXPAcBqWZUrOQC4z0QOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLG2LXuADXHE9tSJJy57ii3pL179+mWPsGUd95ojlz3ClvZDZ//kskfYui67ZI+7XMkBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATCWyAEwlsgBMJbIATDWfolcVT2rqi64l/0nV9Uz9+E851bVH+2PmQBgv0Suu9/Z3Rfey0NOTrLXyAHA/rTXyFXVsVV1c1VdXFWfrKq3VNXTquqKqvrXqjp1/RVYVT2vqj5WVTdU1WVVdWiSVyc5u6qur6qzF8dcVVXXVdWVVfU9u/m+P7J4zEOq6umL7Y9U1SVVddT+XwoAptnXK7lHJ/m9JMcv/nl+kh9Icn6SX9nlsa9M8sPdfVKSZ3X3fy/ue2t3n9zdb01yc5LTu/t7F/t+Z/0JquqsJBfkW1d/r0jytO7+viTXJHnZfXqWAGxJ2/bxcbd090eTpKpuSnJpd3dVfTTJsbs89ookF1fVXyd5xx7Od0ySN1XVY5J0kkPW7TsjyY4kT+/ur1TVmUlOSHJFVSXJoUmu2vWEVbUzyc4k2X7oMfv4tACYbF+v5O5Yt333utt3Z5dQdveLs3bl9Ygk11bVg3dzvt9M8v7uflySH02yfd2+TyU5Oslxi9uV5L2Lq8CTu/uE7j5v1xN290XdvaO7dxyy7Yh9fFoATLbff4Sgqh7V3Vd39yuTfD5rsbsta+G6xzFJPrPYPneXU3w6yY8neXNVnZjkg0meXFWPXpz/yKo6LgCwFxvxc3Kvq6qPVtXHklyZ5IYk709ywj0fPEny2iSvqarrspuXTLv75iTnJLkkyQOyFsK/rKobs/ZS5fEbMDcAw+z1Pbnu/rckj1t3+9w97Lt4cd9zdnOaW5Ocsst966/GXrE49uJ157kua+/FJWsvYe56PADcK7/xBICxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLFEDoCxRA6AsUQOgLG2LXuAjVB3fjMH/8fnlz3GlvSzL/i5ZY+wZfXBtewRtrSD7r572SOwG67kABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYa9uyB9hfqmpnkp1Jsv3go5Y8DQCbwZgrue6+qLt3dPeOQw86fNnjALAJjIkcAOxq5SJXVe+qqocvew4ANr+Ve0+uu5+57BkAWA0rdyUHAPtK5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGEvkABhL5AAYS+QAGKu6e9kz7HdV9fkkn172HPfDQ5J8YdlDbFHWfnms/fKs+to/srsfursdIyO36qrqmu7esew5tiJrvzzWfnkmr72XKwEYS+QAGEvkNqeLlj3AFmbtl8faL8/YtfeeHABjuZIDYCyRA2AskQNgLJEDYCyRA2Cs/wH7gc2I6w/x0wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x2160 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QexCaZNn_btF"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}